{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         transaction\n",
      "0  jarrett allen explains ethical basketball farm...\n",
      "1  gmo transition positive buzzword detail packag...\n",
      "2                       gmo marine sexual dimorphism\n",
      "3  ysk term essentially advertising ploy gmos est...\n",
      "4                                  gmo bad unhealthy\n",
      "5  years gmo crops results every extra spent gmo ...\n",
      "6  bioengineer founded venture backed company mak...\n",
      "7  scientists genetically modified cassava staple...\n",
      "8         first gmo mosquitoes released florida keys\n",
      "9  years data confirm gmo corn increase crop yiel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\varun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\varun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def text_to_transaction(text, label):\n",
    "    \n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    tokens_cleaned = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    \n",
    "    \n",
    "    return tokens_cleaned\n",
    "\n",
    "\n",
    "with open(\"reddit_posts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "transactions = []\n",
    "for article in articles:\n",
    "    \n",
    "    text = \"\"\n",
    "    if article.get(\"title\"):\n",
    "        text += article.get(\"title\") + \" \"\n",
    "    if article.get(\"description\"):\n",
    "        text += article.get(\"description\")\n",
    "    \n",
    "    label = article.get(\"label\", \"unknown\")\n",
    "    transaction = text_to_transaction(text, label)\n",
    "    transactions.append(transaction)\n",
    "\n",
    "\n",
    "transactions_str = [\" \".join(tokens) for tokens in transactions]\n",
    "\n",
    "\n",
    "df_transactions = pd.DataFrame({'transaction': transactions_str})\n",
    "\n",
    "df_transactions.to_csv(\"transactions_data2.csv\", index=False)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "print(df_transactions.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
